{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "shw1WW4dydI9"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language. It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69_55Z31y6rS",
        "outputId": "bdcf3e98-9d7f-41b7-ceb2-d93133c4e6da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language. It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-x9sov9zpLY",
        "outputId": "e7ba8cda-5438-464a-d682-cf487b5b2bba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##tokenization\n",
        "## sentence --> Paragraph\n",
        "from nltk.tokenize import sent_tokenize           #tokenize the corpus in sentences."
      ],
      "metadata": {
        "id": "zuHj8gKhzAR7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "i0t1pLiczSeE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvluwT67zW_k",
        "outputId": "5338a033-6b6c-4443-d34f-97e808289208"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKLum-RL0LuU",
        "outputId": "8ab35af2-3c5c-4486-fee0-514e6940acdd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language.\n",
            "It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##tokenization\n",
        "## Paragraph --> words\n",
        "##sentence --> words\n",
        "from nltk.tokenize import word_tokenize          #tokenize the corpus in words."
      ],
      "metadata": {
        "id": "_Sju8qIk0RTL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VKQ4zg90nF9",
        "outputId": "0fc93378-9b64-4213-eccc-da5864337b7b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Toolkit',\n",
              " ',',\n",
              " 'or',\n",
              " 'more',\n",
              " 'commonly',\n",
              " 'NLTK',\n",
              " ',',\n",
              " 'is',\n",
              " 'a',\n",
              " 'suite',\n",
              " 'of',\n",
              " 'libraries',\n",
              " 'and',\n",
              " 'programs',\n",
              " 'for',\n",
              " 'symbolic',\n",
              " 'and',\n",
              " 'statistical',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'for',\n",
              " 'English',\n",
              " 'written',\n",
              " 'in',\n",
              " 'the',\n",
              " 'Python',\n",
              " 'programming',\n",
              " 'language',\n",
              " '.',\n",
              " 'It',\n",
              " 'supports',\n",
              " 'classification',\n",
              " ',',\n",
              " 'tokenization',\n",
              " ',',\n",
              " 'stemming',\n",
              " ',',\n",
              " 'tagging',\n",
              " ',',\n",
              " 'parsing',\n",
              " ',',\n",
              " 'and',\n",
              " 'semantic',\n",
              " 'reasoning',\n",
              " 'functionalities',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX0dfE6e0sWL",
        "outputId": "f2d7606d-f99a-470c-ec2e-8d7f59d003f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Natural', 'Language', 'Toolkit', ',', 'or', 'more', 'commonly', 'NLTK', ',', 'is', 'a', 'suite', 'of', 'libraries', 'and', 'programs', 'for', 'symbolic', 'and', 'statistical', 'natural', 'language', 'processing', 'for', 'English', 'written', 'in', 'the', 'Python', 'programming', 'language', '.']\n",
            "['It', 'supports', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', 'functionalities', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize # this also tokenize punctuations"
      ],
      "metadata": {
        "id": "NtKnrpr109mD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(wordpunct_tokenize(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WswBy0ig10lT",
        "outputId": "e4889082-f2d4-473f-cfd1-3d4cae0208a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Natural', 'Language', 'Toolkit', ',', 'or', 'more', 'commonly', 'NLTK', ',', 'is', 'a', 'suite', 'of', 'libraries', 'and', 'programs', 'for', 'symbolic', 'and', 'statistical', 'natural', 'language', 'processing', 'for', 'English', 'written', 'in', 'the', 'Python', 'programming', 'language', '.']\n",
            "['It', 'supports', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', 'functionalities', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Stemming</h1>"
      ],
      "metadata": {
        "id": "bB1Dmw8P5J4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP)."
      ],
      "metadata": {
        "id": "rfamhkFn5Qin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = [\"playing\", \"dancing\", \"gone\", \"loves\", \"games\", \"playing\",\"controlable\",\"buses\"]"
      ],
      "metadata": {
        "id": "nmK3b52l5T-a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "Hck6e30K5_yy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer  = PorterStemmer()"
      ],
      "metadata": {
        "id": "z8kpJEYs6y-_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word:\n",
        "    print(i + \"   ---->   \" + stemmer.stem(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v743E2xw6a-L",
        "outputId": "6f294071-d7b6-4f4a-dee2-d6ba36c22d17"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "playing   ---->   play\n",
            "dancing   ---->   danc\n",
            "gone   ---->   gone\n",
            "loves   ---->   love\n",
            "games   ---->   game\n",
            "playing   ---->   play\n",
            "controlable   ---->   control\n",
            "buses   ---->   buse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>RegexpStemmer class</h1>\n",
        "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression. Let us see an example"
      ],
      "metadata": {
        "id": "vqkBkJVo-3UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "zuXfPqtb7qcy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regex = RegexpStemmer('ing$|s$|es$|able$|ne$', min=4)"
      ],
      "metadata": {
        "id": "v0LZpanU_JGZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word:\n",
        "    print(i + \"   ---->   \" + regex.stem(i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoUbQJus_UyZ",
        "outputId": "880e43cb-1412-430d-f6b1-0fd460c580ad"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "playing   ---->   play\n",
            "dancing   ---->   danc\n",
            "gone   ---->   go\n",
            "loves   ---->   lov\n",
            "games   ---->   gam\n",
            "playing   ---->   play\n",
            "controlable   ---->   control\n",
            "buses   ---->   bus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Snowball Stemmer</h1>\n",
        "It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer."
      ],
      "metadata": {
        "id": "tGGkzLbLAItw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "8fdWdgUUAHQ3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "uQ1lFasA_vpa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word:\n",
        "    print(i + \"   ---->   \" + snowball.stem(i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdsZ6R2fAUfy",
        "outputId": "ce78c57e-459a-48ae-ffc0-9cd68a5e3dcf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "playing   ---->   play\n",
            "dancing   ---->   danc\n",
            "gone   ---->   gone\n",
            "loves   ---->   love\n",
            "games   ---->   game\n",
            "playing   ---->   play\n",
            "controlable   ---->   control\n",
            "buses   ---->   buse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Lemmatizer</h1>"
      ],
      "metadata": {
        "id": "4-Iuyj3nBGQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Wordnet Lemmatizer</h1>\n",
        "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n",
        "\n",
        "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma. Let us understand it with an example."
      ],
      "metadata": {
        "id": "f2tCUro6BOo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wfB6X7XDO-i",
        "outputId": "e3fe41a0-c875-47f2-e470-ed7e05337599"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "ophXDVgZAngp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "POS- Noun-n\n",
        "verb-v\n",
        "adjective-a\n",
        "adverb-r\n",
        "'''\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "gSl71utQBdVZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word:\n",
        "    print(i + \"   ---->   \" + lemmatizer.lemmatize(i, pos= 'v'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2FYtGGHC495",
        "outputId": "34c490bb-9c03-4912-e0e7-7bba9e0442da"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "playing   ---->   play\n",
            "dancing   ---->   dance\n",
            "gone   ---->   go\n",
            "loves   ---->   love\n",
            "games   ---->   game\n",
            "playing   ---->   play\n",
            "controlable   ---->   controlable\n",
            "buses   ---->   bus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Stopword</h1>"
      ],
      "metadata": {
        "id": "pZ7JP7YREbLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apj = \"\"\"I have three visions for India. In 3000 years of our history, people from all over the world have come and invaded us, captured our lands, conquered our minds. From Alexander onwards, The Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours. Yet we have not done this to any other nation. We have not conquered anyone. We have not grabbed their land, their culture, their history and Tried to enforce our way of life on them. Why? Because we respect the freedom of others.\n",
        "\n",
        "That is why my first vision is that of FREEDOM. I believe that India got its first vision of this in 1857, when we started the war of Independence. It is this freedom that we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "\n",
        "My second vision for India's DEVELOPMENT, For fifty years we have been A developing nation. It is time we see ourselves as a developed nation. We are among top 5 nations of the world in terms of GDP. We have 10 percent growth rate in most areas. Our poverty levels are falling. Our achievements are being globally recognized today. Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. Isn't this incorrect?\n",
        "\n",
        "I have a THIRD vision. India must stand up to the world. Because I believe that, unless India stands up to the world, no one will respect us. Only strength respects strength. We must be strong not only as a military power but also as an economic power. Both must go hand-in-hand. My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of space, Professor Satish Dhawan, who succeeded him and Dr.Brahm Prakash, father of nuclear material. I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\"\"\""
      ],
      "metadata": {
        "id": "Ns_mdX88C-Co"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3pTN8vAFk2f",
        "outputId": "5fff9b8f-2f59-4cbd-a498-d25005918e3d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop = stopwords.words('english')\n",
        "print(stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-rZZ5SkFwqR",
        "outputId": "46d70e93-fa5b-49d6-9734-aaebcb6222ae"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apj =sent_tokenize(apj)\n"
      ],
      "metadata": {
        "id": "qPTYYjFYGNef"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(apj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zX20AOsHD07",
        "outputId": "b0e122d7-c865-4ec7-d297-2d361d88b878"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-E3jdcMHnVw",
        "outputId": "5c902524-76be-4ccd-b3bd-ca8855011c89"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards, The Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.',\n",
              " 'We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture, their history and Tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.',\n",
              " 'That is why my first vision is that of FREEDOM.',\n",
              " 'I believe that India got its first vision of this in 1857, when we started the war of Independence.',\n",
              " 'It is this freedom that we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.',\n",
              " \"My second vision for India's DEVELOPMENT, For fifty years we have been A developing nation.\",\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among top 5 nations of the world in terms of GDP.',\n",
              " 'We have 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognized today.',\n",
              " 'Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " \"Isn't this incorrect?\",\n",
              " 'I have a THIRD vision.',\n",
              " 'India must stand up to the world.',\n",
              " 'Because I believe that, unless India stands up to the world, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be strong not only as a military power but also as an economic power.',\n",
              " 'Both must go hand-in-hand.',\n",
              " 'My good fortune was to have worked with three great minds.',\n",
              " 'Dr. Vikram Sarabhai of the Dept.',\n",
              " 'of space, Professor Satish Dhawan, who succeeded him and Dr.Brahm Prakash, father of nuclear material.',\n",
              " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwords And Filter And then Apply stemming\n",
        "\n",
        "for i in range(len(apj)):\n",
        "    words=nltk.word_tokenize(apj[i])\n",
        "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    apj[i]=' '.join(words)# converting all the list of words into sentences"
      ],
      "metadata": {
        "id": "OxNGAOkmJzPr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXX8CqTQJ7s6",
        "outputId": "6ab6c4ac-2b53-44d2-932c-997809a718db"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i three vision india .',\n",
              " 'in 3000 year histori , peopl world come invad u , captur land , conquer mind .',\n",
              " 'from alexand onward , the greek , turk , mogul , portugues , british , french , dutch , come loot u , take .',\n",
              " 'yet nation .',\n",
              " 'we conquer anyon .',\n",
              " 'we grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom other .',\n",
              " 'that first vision freedom .',\n",
              " 'i believ india get first vision 1857 , start war independ .',\n",
              " 'it freedom must protect nurtur build .',\n",
              " 'if free , one respect u .',\n",
              " \"my second vision india 's develop , for fifti year a develop nation .\",\n",
              " 'it time see develop nation .',\n",
              " 'we among top 5 nation world term gdp .',\n",
              " 'we 10 percent growth rate area .',\n",
              " 'our poverti level fall .',\n",
              " 'our achiev global recogn today .',\n",
              " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
              " \"is n't incorrect ?\",\n",
              " 'i third vision .',\n",
              " 'india must stand world .',\n",
              " 'becaus i believ , unless india stand world , one respect u .',\n",
              " 'onli strength respect strength .',\n",
              " 'we must strong militari power also econom power .',\n",
              " 'both must go hand-in-hand .',\n",
              " 'my good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr.brahm prakash , father nuclear materi .',\n",
              " 'i lucki work three close consid great opportun life .']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwords And Filter And then Apply lemmatizer\n",
        "\n",
        "for i in range(len(apj)):\n",
        "    words=nltk.word_tokenize(apj[i])\n",
        "    words=[lemmatizer.lemmatize(word, pos = 'v') for word in words if word not in set(stopwords.words('english'))]\n",
        "    apj[i]=' '.join(words)# converting all the list of words into sentences"
      ],
      "metadata": {
        "id": "aV5CnafKHpIQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC3ExRiwJiRs",
        "outputId": "8737bfe5-f162-403b-e97b-a1583d3e7607"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I three vision India .',\n",
              " 'In 3000 year history , people world come invade u , capture land , conquer mind .',\n",
              " 'From Alexander onwards , The Greeks , Turks , Moguls , Portuguese , British , French , Dutch , come loot u , take .',\n",
              " 'Yet do nation .',\n",
              " 'We conquer anyone .',\n",
              " 'We grab land , culture , history Tried enforce way life .',\n",
              " 'Why ?',\n",
              " 'Because respect freedom others .',\n",
              " 'That first vision FREEDOM .',\n",
              " 'I believe India get first vision 1857 , start war Independence .',\n",
              " 'It freedom must protect nurture build .',\n",
              " 'If free , one respect u .',\n",
              " \"My second vision India 's DEVELOPMENT , For fifty year A develop nation .\",\n",
              " 'It time see develop nation .',\n",
              " 'We among top 5 nation world term GDP .',\n",
              " 'We 10 percent growth rate area .',\n",
              " 'Our poverty level fall .',\n",
              " 'Our achievement globally recognize today .',\n",
              " 'Yet lack self-confidence see develop nation , self-reliant self-assured .',\n",
              " \"Is n't incorrect ?\",\n",
              " 'I THIRD vision .',\n",
              " 'India must stand world .',\n",
              " 'Because I believe , unless India stand world , one respect u .',\n",
              " 'Only strength respect strength .',\n",
              " 'We must strong military power also economic power .',\n",
              " 'Both must go hand-in-hand .',\n",
              " 'My good fortune work three great mind .',\n",
              " 'Dr. Vikram Sarabhai Dept .',\n",
              " 'space , Professor Satish Dhawan , succeed Dr.Brahm Prakash , father nuclear material .',\n",
              " 'I lucky work three closely consider great opportunity life .']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>POS TAGGING</h1>"
      ],
      "metadata": {
        "id": "QV21FAHSPzWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFdYy157JmUM",
        "outputId": "6981adaa-2566-4a60-ac6b-47489a7894a1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(apj)):\n",
        "    words=nltk.word_tokenize(apj[i])\n",
        "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
        "    pos_tag = nltk.pos_tag(words)\n",
        "    print(pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1rcaetLQPhu",
        "outputId": "96be3fad-f630-4505-e931-a2f1a02f7c88"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('three', 'CD'), ('vision', 'NN'), ('india', 'NN'), ('.', '.')]\n",
            "[('3000', 'CD'), ('year', 'NN'), ('histori', 'NN'), (',', ','), ('peopl', 'JJ'), ('world', 'NN'), ('come', 'VBP'), ('invad', 'NN'), ('u', 'NN'), (',', ','), ('captur', 'NN'), ('land', 'NN'), (',', ','), ('conquer', 'NN'), ('mind', 'NN'), ('.', '.')]\n",
            "[('alexand', 'RB'), ('onward', 'RB'), (',', ','), ('greek', 'JJ'), (',', ','), ('turk', 'NN'), (',', ','), ('mogul', 'NN'), (',', ','), ('portugues', 'NNS'), (',', ','), ('british', 'JJ'), (',', ','), ('french', 'JJ'), (',', ','), ('dutch', 'VB'), (',', ','), ('come', 'VB'), ('loot', 'NN'), ('u', 'JJ'), (',', ','), ('take', 'VB'), ('.', '.')]\n",
            "[('yet', 'RB'), ('nation', 'NN'), ('.', '.')]\n",
            "[('conquer', 'NN'), ('anyon', 'NN'), ('.', '.')]\n",
            "[('grab', 'NN'), ('land', 'NN'), (',', ','), ('cultur', 'NN'), (',', ','), ('histori', 'JJ'), ('tri', 'NN'), ('enforc', 'NN'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
            "[('whi', 'NN'), ('?', '.')]\n",
            "[('becaus', 'NN'), ('respect', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
            "[('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
            "[('believ', 'NN'), ('india', 'NN'), ('get', 'VBP'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('start', 'VBP'), ('war', 'NN'), ('independ', 'NN'), ('.', '.')]\n",
            "[('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurtur', 'JJ'), ('build', 'NN'), ('.', '.')]\n",
            "[('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('u', 'NN'), ('.', '.')]\n",
            "[('second', 'JJ'), ('vision', 'NN'), ('india', 'NN'), (\"'s\", 'POS'), ('develop', 'NN'), (',', ','), ('fifti', 'JJ'), ('year', 'NN'), ('develop', 'NN'), ('nation', 'NN'), ('.', '.')]\n",
            "[('time', 'NN'), ('see', 'VB'), ('develop', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
            "[('among', 'IN'), ('top', 'JJ'), ('5', 'CD'), ('nation', 'NN'), ('world', 'NN'), ('term', 'NN'), ('gdp', 'NN'), ('.', '.')]\n",
            "[('10', 'CD'), ('percent', 'JJ'), ('growth', 'NN'), ('rate', 'NN'), ('area', 'NN'), ('.', '.')]\n",
            "[('poverti', 'JJ'), ('level', 'NN'), ('fall', 'NN'), ('.', '.')]\n",
            "[('achiev', 'NN'), ('global', 'JJ'), ('recogn', 'NN'), ('today', 'NN'), ('.', '.')]\n",
            "[('yet', 'RB'), ('lack', 'JJ'), ('self-confid', 'JJ'), ('see', 'NN'), ('develop', 'VB'), ('nation', 'NN'), (',', ','), ('self-reli', 'JJ'), ('self-assur', 'NN'), ('.', '.')]\n",
            "[(\"n't\", 'RB'), ('incorrect', 'VB'), ('?', '.')]\n",
            "[('third', 'JJ'), ('vision', 'NN'), ('.', '.')]\n",
            "[('india', 'NN'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
            "[('becaus', 'NN'), ('believ', 'NN'), (',', ','), ('unless', 'IN'), ('india', 'JJ'), ('stand', 'NN'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('u', 'NN'), ('.', '.')]\n",
            "[('onli', 'JJ'), ('strength', 'NN'), ('respect', 'NN'), ('strength', 'NN'), ('.', '.')]\n",
            "[('must', 'MD'), ('strong', 'JJ'), ('militari', 'NN'), ('power', 'NN'), ('also', 'RB'), ('econom', 'JJ'), ('power', 'NN'), ('.', '.')]\n",
            "[('must', 'MD'), ('go', 'VB'), ('hand-in-hand', 'NN'), ('.', '.')]\n",
            "[('good', 'JJ'), ('fortun', 'NN'), ('work', 'NN'), ('three', 'CD'), ('great', 'JJ'), ('mind', 'NN'), ('.', '.')]\n",
            "[('dr.', 'NN'), ('vikram', 'NN'), ('sarabhai', 'NN'), ('dept', 'NN'), ('.', '.')]\n",
            "[('space', 'NN'), (',', ','), ('professor', 'NN'), ('satish', 'JJ'), ('dhawan', 'NN'), (',', ','), ('succeed', 'VB'), ('dr.brahm', 'JJ'), ('prakash', 'NN'), (',', ','), ('father', 'RB'), ('nuclear', 'JJ'), ('materi', 'NN'), ('.', '.')]\n",
            "[('lucki', 'NN'), ('work', 'NN'), ('three', 'CD'), ('close', 'JJ'), ('consid', 'NN'), ('great', 'JJ'), ('opportun', 'JJ'), ('life', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"i love to play basketball\"\n",
        "words = nltk.word_tokenize(sent)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voNnIU0gRMOv",
        "outputId": "45b1bb28-3d11-40b1-a400-82456647b639"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'love', 'to', 'play', 'basketball']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the POS tags for the words\n",
        "pos_tags = nltk.pos_tag_sents([words])\n",
        "\n",
        "# Print the POS tags\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIk8kHgwSz6z",
        "outputId": "0b89bd3b-0a77-46e7-c359-52649906ad15"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('i', 'NN'), ('love', 'VBP'), ('to', 'TO'), ('play', 'VB'), ('basketball', 'NN')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Named Entity Recognization</h1>"
      ],
      "metadata": {
        "id": "ZvY9OX1VUClv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY017xAPUCRY",
        "outputId": "23f2278c-b15c-49c9-a859-09f8bf58ab16"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxuLb3QdS3qO",
        "outputId": "c245cf69-9bb0-4b68-9c84-d7ed78dac796"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\"\" The oldest classical British and Latin writings had little or no space between words and could be written in boustrophedon (alternating directions). Over time, text direction (left to right) became standardized. Word dividers and terminal punctuation became common. The first way to divide sentences into groups was the original paragraphos, similar to an underscore at the beginning of the new group.[2] The Greek parágraphos evolved into the pilcrow (¶), which in English manuscripts in the Middle Ages can be seen inserted inline between sentences.\"\"\""
      ],
      "metadata": {
        "id": "ytbZfP6ZUgJu"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=nltk.word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "2EGKwPbaVCEl"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_elements=nltk.pos_tag(words)\n"
      ],
      "metadata": {
        "id": "hItM1AqxVTD1"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.ne_chunk(tag_elements).draw()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "Sfph5-MSVaac",
        "outputId": "01cda757-fc52-4b8e-afa0-0692ab0bb15c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "no display name and no $DISPLAY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-bbdad5f7ef3a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tree/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *trees)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NLTK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<Control-x>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2297\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vFbqmXKCVdmk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}